{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Java Plagiarism Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Lexer analyzer and source code to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    }
   ],
   "source": [
    "print(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JWZPOS1nJLH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from java_lexer import Lexer\n",
    "from source_codes import SOURCE_CODE2, SOURCE_CODE1, SOURCE_CODE3, SOURCE_CODE4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Lexer class and obtain all tokens from source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer1 = Lexer(SOURCE_CODE3)\n",
    "tokens1 = lexer1.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer2 = Lexer(SOURCE_CODE4)\n",
    "tokens2 = lexer2.tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the identified tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los tokens obtenidos a un corpus de texto\n",
    "\n",
    "code1_text = ' '.join(token.type.join(token.value) for token in tokens1)\n",
    "\n",
    "code2_text = ' '.join(token.type.join(token.value) for token in tokens2)\n",
    "\n",
    "#code1_text = ' '.join(token.type for token in tokens1)\n",
    "\n",
    "#code2_text = ' '.join(token.type for token in tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "# Fit and transform code 1\n",
    "X1 = vectorizer.fit_transform([code1_text])\n",
    "code1_vector = X1.toarray()[0]\n",
    "\n",
    "# Transform code 2\n",
    "X2 = vectorizer.transform([code2_text])\n",
    "code2_vector = X2.toarray()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8748001137288177\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity using cosine similarity\n",
    "similarity = cosine_similarity([code1_vector], [code2_vector])[0][0]\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Segment:\n",
      "public class TrueFalseExcercise { public static void main ( String [ ] args ) { String nadaquever = \"nadaquever\" ; String nadaquever = \"nadaquever\" ; String nadaquever = \"nadaquever\" ; String nadaquever = \"nadaquever\" ; // Utilizando los valores booleanos en operaciones booleanas boolean isTrue = true ; boolean isFalse = false ; boolean result1 = isTrue && isFalse ; boolean result2 = isTrue | | isFalse ; boolean result3 = ! isFalse ; System.out.println ( \" 1: \" + result1 ) ; System.out.println ( \" 3: \" + result3 ) ; System.out.println ( \" 2: \" + result2 ) ; } }\n",
      "Similarity: 0.8748001137288177\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform code 1\n",
    "X1 = vectorizer.fit_transform([code1_text])\n",
    "code1_vector = X1.toarray()[0]\n",
    "\n",
    "# Transform code 2\n",
    "X2 = vectorizer.transform([code2_text])\n",
    "code2_vector = X2.toarray()[0]\n",
    "\n",
    "# Calculate similarity using cosine similarity\n",
    "similarity_matrix = cosine_similarity([code1_vector], [code2_vector])\n",
    "\n",
    "# Find the segments with the highest similarity\n",
    "max_similarity_index = similarity_matrix.argmax()\n",
    "max_similarity_value = similarity_matrix[0][max_similarity_index]\n",
    "\n",
    "# Find the corresponding segments in the code\n",
    "segment_length = len(tokens1)\n",
    "start_index = max_similarity_index\n",
    "end_index = start_index + segment_length\n",
    "matched_segment = tokens2[start_index:end_index]\n",
    "\n",
    "# Print the matched segment and similarity value\n",
    "print(\"Matched Segment:\")\n",
    "print(\" \".join(token.value for token in matched_segment ))\n",
    "print(\"Similarity:\", max_similarity_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import java.util.ArrayList;\n",
      "import java.util.List;\n",
      "\n",
      "// This is a single-line comment\n",
      "class HelloWorld {\n",
      "    public static void main(String[] args) {\n",
      "        /* This is a\n",
      "           multi-line comment */\n",
      "        boolean flag = true;\n",
      "        String message = \"Hello, World!\";\n",
      "        int number = 123;\n",
      "        double pi = 3.14;\n",
      "        int[] array = {1, 2, 3};\n",
      "        if (flag == true) {\n",
      "            System.out.println(message);\n",
      "        }\n",
      "        else {\n",
      "            System.out.println(\"Flag is false.\");\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "IMPORT: import java.util.ArrayList;\n",
      "IMPORT: import java.util.List;\n",
      "COMMENT_SINGLE: // This is a single-line comment\n",
      "KEYWORD: class\n",
      "IDENTIFIER: HelloWorld\n",
      "PUNCTUATION: {\n",
      "KEYWORD: public\n",
      "KEYWORD: static\n",
      "DATA_TYPE: void\n",
      "IDENTIFIER: main\n",
      "PUNCTUATION: (\n",
      "IDENTIFIER: String\n",
      "PUNCTUATION: [\n",
      "PUNCTUATION: ]\n",
      "IDENTIFIER: args\n",
      "PUNCTUATION: )\n",
      "PUNCTUATION: {\n",
      "COMMENT_MULTI: /* This is a\n",
      "           multi-line comment */\n",
      "DATA_TYPE: boolean\n",
      "IDENTIFIER: flag\n",
      "OPERATOR: =\n",
      "BOOLEAN: true\n",
      "PUNCTUATION: ;\n",
      "IDENTIFIER: String\n",
      "IDENTIFIER: message\n",
      "OPERATOR: =\n",
      "STRING: \"Hello, World!\"\n",
      "PUNCTUATION: ;\n",
      "DATA_TYPE: int\n",
      "IDENTIFIER: number\n",
      "OPERATOR: =\n",
      "NUMBER: 123\n",
      "PUNCTUATION: ;\n",
      "DATA_TYPE: double\n",
      "IDENTIFIER: pi\n",
      "OPERATOR: =\n",
      "NUMBER: 3.14\n",
      "PUNCTUATION: ;\n",
      "DATA_TYPE: int\n",
      "PUNCTUATION: [\n",
      "PUNCTUATION: ]\n",
      "IDENTIFIER: array\n",
      "OPERATOR: =\n",
      "PUNCTUATION: {\n",
      "NUMBER: 1\n",
      "PUNCTUATION: ,\n",
      "NUMBER: 2\n",
      "PUNCTUATION: ,\n",
      "NUMBER: 3\n",
      "PUNCTUATION: }\n",
      "PUNCTUATION: ;\n",
      "KEYWORD: if\n",
      "PUNCTUATION: (\n",
      "IDENTIFIER: flag\n",
      "OPERATOR: =\n",
      "OPERATOR: =\n",
      "BOOLEAN: true\n",
      "PUNCTUATION: )\n",
      "PUNCTUATION: {\n",
      "SYSTEM_CALL: System.out.println\n",
      "PUNCTUATION: (\n",
      "IDENTIFIER: message\n",
      "PUNCTUATION: )\n",
      "PUNCTUATION: ;\n",
      "PUNCTUATION: }\n",
      "KEYWORD: else\n",
      "PUNCTUATION: {\n",
      "SYSTEM_CALL: System.out.println\n",
      "PUNCTUATION: (\n",
      "STRING: \"Flag is false.\"\n",
      "PUNCTUATION: )\n",
      "PUNCTUATION: ;\n",
      "PUNCTUATION: }\n",
      "PUNCTUATION: }\n",
      "PUNCTUATION: }\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from java_lexer import Lexer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Preprocess and tokenize source code 1\n",
    "lexer1 = Lexer(SOURCE_CODE3)\n",
    "tokens1 = lexer1.tokenize()\n",
    "\n",
    "# Preprocess and tokenize source code 2\n",
    "lexer2 = Lexer(SOURCE_CODE4)\n",
    "tokens2 = lexer2.tokenize()\n",
    "\n",
    "# Combine tokens from both source codes\n",
    "corpus = [token.type + token.value for token in tokens1] + [token.type + token.value for token in tokens2]\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = sorted(set(corpus))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Convert code snippets to sequences of indices\n",
    "seq1 = [word_to_idx[token.type + token.value] for token in tokens1]\n",
    "seq2 = [word_to_idx[token.type + token.value] for token in tokens2]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = max(len(seq1), len(seq2))\n",
    "seq1_padded = pad_sequences([seq1], maxlen=max_sequence_length, padding='post')\n",
    "seq2_padded = pad_sequences([seq2], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create LSTM model\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(np.vstack((seq1_padded, seq2_padded)), np.array([0, 1]), epochs=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 487ms/step\n",
      "[[0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Compare the similarity between the two code snippets\n",
    "similarity = model.predict(np.vstack((seq1_padded, seq2_padded)))\n",
    "similarity_classes = (similarity > 0.5).astype(int)\n",
    "print(similarity_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-2-ffa7cfc39f9c>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-ffa7cfc39f9c>\"\u001b[1;36m, line \u001b[1;32m44\u001b[0m\n\u001b[1;33m    model.compile(loss='binary_crosseimport os\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from java_lexer import Lexer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "java_files_folder = \"version_1\"  # Replace with the path to the root folder\n",
    "\n",
    "# Collect tokens from all Java files in the folders\n",
    "corpus = []\n",
    "for root, dirs, files in os.walk(java_files_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".java\"):\n",
    "            print(\"found java file\")\n",
    "            file_path = os.path.join(root, file)\n",
    "            lexer = Lexer(open(file_path, \"r\").read())\n",
    "            tokens = lexer.tokenize()\n",
    "            corpus.extend([token.type + token.value for token in tokens])\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = sorted(set(corpus))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "# Convert code snippets to sequences of indices\n",
    "seq1 = [word_to_idx[str(token.type) + str(token.value)] for token in tokens1]\n",
    "seq2 = [word_to_idx[str(token.type) + str(token.value)] for token in tokens2]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = max(len(seq1), len(seq2))\n",
    "seq1_padded = pad_sequences([seq1], maxlen=max_sequence_length, padding='post')\n",
    "seq2_padded = pad_sequences([seq2], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create LSTM model\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(sequences_padded, np.array([0]*len(sequences_padded)), epochs=10)  # Use dummy labels for training\n",
    "\n",
    "# Compare the similarity between code snippets\n",
    "similarity = model.predict(sequences_padded)\n",
    "similarity_classes = (similarity > 0.5).astype(int)\n",
    "print(similarity_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-de6b4e06ef65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjava_lexer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cutic\\OneDrive\\Escritorio\\Tecnológico de Monterrey\\Octavo semestre\\Desarrollo de aplicaciones avanzadas de ciencias computacionales\\JavaAIPlagiarismDetector\\java_lexer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;31m# Mandar a llamar el lexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[0mlexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;31m# Imprimir el resultado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cutic\\OneDrive\\Escritorio\\Tecnológico de Monterrey\\Octavo semestre\\Desarrollo de aplicaciones avanzadas de ciencias computacionales\\JavaAIPlagiarismDetector\\java_lexer.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTOKEN_TYPES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cutic\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from java_lexer import Lexer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "from java_lexer import Lexer\n",
    "from source_codes import SOURCE_CODE2, SOURCE_CODE1, SOURCE_CODE3, SOURCE_CODE4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_files_folder = \"vocab_train\"  # Replace with the path to the root folder\n",
    "\n",
    "# Collect tokens from all Java files in the folders\n",
    "corpus = []\n",
    "file_count = 0\n",
    "print (\"iniciando lectura de archivos\")\n",
    "for root, dirs, files in os.walk(java_files_folder):\n",
    "    for file in files:\n",
    "        print(\"file\")\n",
    "        if file.endswith(\".java\"):\n",
    "            print(\"found %s\", file)\n",
    "            if file_count >= 50:\n",
    "                break\n",
    "            file_count += 1\n",
    "            file_path = os.path.join(root, file)\n",
    "            lexer = Lexer(open(file_path, \"r\", encoding=\"utf8\").read())\n",
    "            tokens = lexer.tokenize()\n",
    "            corpus.extend([token.type + token.value for token in tokens])\n",
    "\n",
    "print(\"finished reading files\")\n",
    "# Create a vocabulary\n",
    "vocab = sorted(set(corpus))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: index for index, word in enumerate(vocab)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert code snippets to sequences of indices\n",
    "seq1 = [word_to_idx[str(token.type) + str(token.value)] for token in tokens1]\n",
    "seq2 = [word_to_idx[str(token.type) + str(token.value)] for token in tokens2]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = max(len(seq1), len(seq2))\n",
    "seq1_padded = pad_sequences([seq1], maxlen=max_sequence_length, padding='post')\n",
    "seq2_padded = pad_sequences([seq2], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create LSTM model\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(sequences_padded, np.array([0]*len(sequences_padded)), epochs=10)  # Use dummy labels for training\n",
    "\n",
    "# Compare the similarity between code snippets\n",
    "similarity = model.predict(sequences_padded)\n",
    "similarity_classes = (similarity > 0.5).astype(int)\n",
    "print(similarity_classes)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
